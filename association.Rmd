# Association tests

Since TOPMed has many studies with related participants, we focus on linear mixed models. Logistic mixed models are also possible using GENESIS, see the [GMMAT paper](https://www.ncbi.nlm.nih.gov/pubmed/27018471).

## Null model

The first step in an association test is to fit the null model. We will need an `AnnotatedDataFrame` with phenotypes, and a GRM. We have a sample annotation with a `sample.id` column matched to the GDS file, and a phenotype file with `subject_id`. (In this example, we use the 1000 Genomes IDs for both sample and subject ID.) For TOPMed data, it is also important to match by study, as subject IDs are not unique across studies.

```{r null_model}
# sample annotation
workshop.path <- "https://github.com/UW-GAC/topmed_workshop_2018/raw/master"
sampfile <- "data/sample_annotation.RData"
if (!dir.exists("data")) dir.create("data")
if (!file.exists(sampfile)) download.file(file.path(workshop.path, sampfile), sampfile)
annot <- TopmedPipeline::getobj(sampfile)
library(Biobase)
head(pData(annot))

# phenotypes by subject ID
phenfile <- "data/phenotype_annotation.RData"
if (!file.exists(phenfile)) download.file(file.path(workshop.path, phenfile), phenfile)
phen <- TopmedPipeline::getobj(phenfile)
head(pData(phen))
varMetadata(phen)

# merge sample annotation with phenotypes
library(dplyr)
dat <- pData(annot) %>%
    left_join(pData(phen), by=c("subject.id"="subject_id", "sex"="sex"))
meta <- bind_rows(varMetadata(annot), varMetadata(phen)[3:5,,drop=FALSE])
annot <- AnnotatedDataFrame(dat, meta)
save(annot, file="data/sample_phenotype_annotation.RData")

# load the GRM
data.path <- "https://github.com/UW-GAC/analysis_pipeline/raw/master/testdata"
grmfile <- "data/grm.RData"
if (!file.exists(grmfile)) download.file(file.path(data.path, basename(grmfile)), grmfile)
grm <- TopmedPipeline::getobj(grmfile)
# the row and column names of the covariance matrix must be set to sample.id
rownames(grm$grm) <- colnames(grm$grm) <- grm$sample.id
```

We will test for an association between genotype and height, adjusting for sex, age, and study as covariates. If the sample set involves multiple distinct groups with different variances for the phenotype, we recommend allowing the model to use heterogeneous variance among groups with the parameter `group.var`. We saw in a previous exercise that the variance differs by study.

```{r null_model_fit}
library(GENESIS)
nullmod <- fitNullModel(annot, outcome="height", covars=c("sex", "age", "study"), 
                        cov.mat=grm$grm, group.var="study", verbose=FALSE)
save(nullmod, file="data/null_model.RData")
```

We also recommend taking an inverse normal transform of the residuals and refitting the model. This is done separately for each group, and the transformed residuals are rescaled. See the full procedure in the  
[pipeline documenation](https://github.com/UW-GAC/analysis_pipeline#association-testing).

## Single-variant tests

Single-variant tests are the same as in GWAS. We use the `assocTestSingle` function in GENESIS. First, we have to create a `SeqVarData` object including both the GDS file and the sample annotation containing phenotypes. We then create a `SeqVarBlockIterator` object to iterate over blocks of variants.

```{r assoc_single}
library(SeqVarTools)
gdsfile <- "data/1KG_phase3_subset_chr1.gds"
if (!file.exists(gdsfile)) download.file(file.path(data.path, basename(gdsfile)), gdsfile)
gds <- seqOpen(gdsfile)
seqData <- SeqVarData(gds, sampleData=annot)
iterator <- SeqVarBlockIterator(seqData, verbose=FALSE)
assoc <- assocTestSingle(iterator, nullmod)
head(assoc)
```

We make a QQ plot to examine the results.

```{r assoc_single_qq}
library(ggplot2)
qqPlot <- function(pval) {
    pval <- pval[!is.na(pval)]
    n <- length(pval)
    x <- 1:n
    dat <- data.frame(obs=sort(pval),
                      exp=x/n,
                      upper=qbeta(0.025, x, rev(x)),
                      lower=qbeta(0.975, x, rev(x)))
    
    ggplot(dat, aes(-log10(exp), -log10(obs))) +
        geom_line(aes(-log10(exp), -log10(upper)), color="gray") +
        geom_line(aes(-log10(exp), -log10(lower)), color="gray") +
        geom_point() +
        geom_abline(intercept=0, slope=1, color="red") +
        xlab(expression(paste(-log[10], "(expected P)"))) +
        ylab(expression(paste(-log[10], "(observed P)"))) +
        theme_bw()
}    

qqPlot(assoc$Score.pval)
```

## Exercises

1. Logistic regression: `fitNullModel` can use a binary phenotype as the outcome variable by specifying the argument `family=binomial`. Use the `status` column in the sample annotation to fit a null model for simulated case/control status, with `sex` and `Population` as covariates. Refer to the documentation for `fitNullModel` to see what other parameters need to be changed for a binary outcome. Then run a single-variant test using this model.

```{r exercise_logistic}
nullmod.status <- fitNullModel(annot, outcome="status", covars=c("sex", "Population"), 
                               cov.mat=grm$grm, family=binomial, verbose=FALSE)
seqResetFilter(seqData, verbose=FALSE)
iterator <- SeqVarBlockIterator(seqData, verbose=FALSE)
assoc <- assocTestSingle(iterator, nullmod.status, test="Score")
head(assoc)
```

2. Inverse normal transform: use the function `nullModelInvNorm` to perform an inverse normal transform on the `height` variable. For each study separately, compute a null model and do the inverse normal transform using just the values for that study. Compare these residuals with the initial residuals you obtained for that study by transforming all studies together.

```{r exercise_invnorm}
nullmod.norm.all <- nullModelInvNorm(nullmod, cov.mat=grm$grm, norm.option="all")
dat.all <- data.frame(sample.id=nullmod.norm.all$sample.id, 
                      resid.norm=nullmod.norm.all$resid.marginal,
                      study=annot$study,
                      run="combined")

nullmod.norm.group <- nullModelInvNorm(nullmod, cov.mat=grm$grm, norm.option="by.group")
dat.group <- data.frame(sample.id=nullmod.norm.group$sample.id, 
                        resid.norm=nullmod.norm.group$resid.marginal,
                        study=annot$study,
                        run="separate")

dat <- rbind(dat.all, dat.group)

ggplot(dat, aes(study, resid.norm, fill=run)) + geom_boxplot()

dat %>% 
    group_by(study, run) %>%
    summarise(mean=mean(resid.norm), var=var(resid.norm))
```

## Sliding window tests

For rare variants, we can do burden tests or SKAT using the GENESIS function `assocTestAggregate`. We restrict the test to variants with alternate allele frequency < 0.1. (For real data, this threshold would be lower.) We use a flat weighting scheme. We define a sliding window across the genome using a `SeqVarWindowIterator`.

```{r assoc_window_burden}
seqResetFilter(seqData, verbose=FALSE)
iterator <- SeqVarWindowIterator(seqData, windowSize=5000, windowShift=2000, verbose=FALSE)
assoc <- assocTestAggregate(iterator, nullmod, test="Burden", AF.max=0.1, weight.beta=c(1,1))
names(assoc)
head(assoc$results)
head(assoc$variantInfo)

qqPlot(assoc$results$Score.pval)
```

For SKAT, we use the Wu weights.

```{r assoc_window_skat}
seqResetFilter(seqData, verbose=FALSE)
iterator <- SeqVarWindowIterator(seqData, windowSize=5000, windowShift=2000, verbose=FALSE)
assoc <- assocTestAggregate(iterator, nullmod, test="SKAT", AF.max=0.1, weight.beta=c(1,25))
head(assoc$results)
head(assoc$variantInfo)

qqPlot(assoc$results$pval_0)
```

### Exercise

Repeat the previous exercise on logistic regression, this time running a sliding-window test.

```{r exercise_sliding}
nullmod.status <- fitNullModel(annot, outcome="status", covars=c("sex", "Population"), 
                               cov.mat=grm$grm, family=binomial, verbose=FALSE)
seqResetFilter(seqData, verbose=FALSE)
iterator <- SeqVarWindowIterator(seqData, windowSize=5000, windowShift=2000, verbose=FALSE)
assoc <- assocTestAggregate(iterator, nullmod, test="SKAT", AF.max=0.1, weight.beta=c(1,25))
head(assoc$results)
```


## Annotation-based aggregate tests

**Note:** the code and libraries in this section are under active development. This section uses [WGSAParsr v 5.0.8](https://github.com/UW-GAC/wgsaparsr/tree/5.0.8) to parse output from [WGSA version 0.7](https://sites.google.com/site/jpopgen/wgsa). Use this code at your own risk, and be warned that it may break in unexpected ways or be incompatible across different versions of the software. [Github issues and contribution](https://github.com/UW-GAC/wgsaparsr) are welcome!

This module is provided to give workshop participants an example of some of the kinds of analysis tasks that might be performed with TOPMed annotation data. 

Analysts generally aggregate rare variants for association testing to decrease multiple testing burden and increase statistical power. They can group variants that fall within arbitrary ranges (such as sliding windows), or they can group variants with intent. For example, an analyst could aggregate variants that that fall between transcription start sites and stop sites, within coding regions, within regulatory regions, or other genomic features selected from sources like published gene models or position- or transcript-based variant annotation. An analyst could also choose to filter the variants prior or subsequent to aggregation using annotation-based criteria such as functional impact or quality scores.

In this workshop, you will aggregate and filter genomic variants using genomic annotation for subsequent association testing. Starting with an annotation file describing 1,922 genomic variants on chromosome 22 from TOPMed's Freeze 5 data release that are also in the 1000 Genomes Project, you will define a configuration file to use the *WGSAParsr* package to select relevant annotation fields, then aggregate the selected variants into genic units and apply filters to restrict the variants in aggreagation units by predicted functional consequence.

### Working with variant annotation
 
Variants called from the TOPMed data set are annotated using the [Whole Genome Sequence Annotator (WGSA)](https://sites.google.com/site/jpopgen/wgsa). Output files from WGSA version 0.7 include 366 annotation fields annotating indel variants, and 438 annotation fields annotating snv variants. In each case, some annotation fields are themselves lists of annotation values. Thus, individual variants may be annotated with more than 1000 individual fields. Not all of these fields will be useful for a particular analysis, and some may be incompatible, so analysts need to parse the WGSA output prior to filtering and aggregation.

The WGSA-annotated variant annotation files we will use for this exercise are available via github:
```{r}
workshop.path <- "https://github.com/UW-GAC/topmed_workshop_2018/raw/master/"

if (!dir.exists("data")) dir.create("data")

snvfile <- "data/snv.tsv.gz"
if (!file.exists(snvfile)) download.file(file.path(workshop.path, snvfile), snvfile)

indelfile <- "data/indel.tsv.gz"
if (!file.exists(indelfile)) download.file(file.path(workshop.path, indelfile), indelfile)
```

Also, you'll be using functions from the `tidyverse` package, so load that library as well:
```{r}
library(tidyverse)
```

WGSA output files are tab-separated text files, with one line per annotated variant. Since there are many annotation fields, these files can be unwieldy to work with directly. As an example, the first two lines of the SNP variant annotation file can be previewed within R:
```{r}
readLines(snvfile, n=2)
```

The TOPMed DCC uses an R package we developed, *WGSAParsr*, to work with WGSA output files. Briefly, WGSAParsr simplifies the WGSA output files by: 1) selecting a subset of fields; 2) renaming some fields; and 3) simplifying fields that have compound list-entries. The *WGSAParsr* package is under development, and is available on github at [https://github.com/UW-GAC/wgsaparsr](https://github.com/UW-GAC/wgsaparsr). It can be installed using the *devtools* package, like this:
```{r eval=FALSE}
# Commented out because you don't need to do this in the workshop - it's already installed on the AMI you're using:

# devtools::install_github("UW-GAC/wgsaparsr", ref = "5.0.8", upgrade_dependencies = FALSE)
```

*note*: if you get an error `Installation failed: error in running command` when you're trying to use `install_github()`, that may be related to some assumptions devtools makes in downloading packages from github. The error can be resolved if you set the unzip option in your R session before running `devtools::install_github()`. Here's how: `options(unzip = "internal")`

Once the package is installed locally, it can be loaded to the workspace in the usual manner:
```{r}
library(wgsaparsr)
```

Then we can begin using tools in the package. `wgsaparsr::get_fields()` lists all of the annotation field headers in a WGSA output file:
```{r}
# list all fields in an annotation file (transpose to make pretty): 
t(get_fields(snvfile))
```

Only a subset of these annotations will be necessary for a particular association test, and it is unweildy to work with all of them, so it is useful to process the WGSA output file to select fields of interest.

An additional complication in working with the WGSA output files is that some of the annotation fields are transcript-based, rather than position-based. Thus, if a variant locus is within multiple transcripts, those fields will have multiple entries (often separated by a `|` character). For example, annotation fields such as `VEP_ensembl_Transcript_ID` may have many values within a single tab-separated field.

WGSAParsr's `parse_to_file()` addresses this by splitting such list-fields into multiple rows. Other annotation fields for that variant are duplicated, and associated columns are filled with the same value for each transcript that a particular variant falls within. A consequence of this approach is that the processed annotation file has more lines than the WGSA output file. In freeze 4, processing expanded the annotation by a factor of about 5 - the 220 million annotations result in a 1-billion row database for subsequent aggregation.

`parse_to_file()` function arguments include a path to a WGSA annotation file, a user-defined configuration file, and output destinations. It reads the input annotation file in chunks, processes them following the specification provided in the user-defined configuration file, and writes to the output destinations. It produces a tab-separated output file useful for subsequent analysis.

The first task, then, is to build the configuration file for `parse_to_file()`.

#### WGSAParsr Configuration 

Details of the configuration file are documented in the `wgsaparsr::load_config()` function documentation, 
```{r eval=FALSE}
?load_config
```

From the documentation, you can see that the configuration file is a tab-separated text file that must have the following columns (in any order): `field`, `SNV`, `indel`, `dbnsfp`, `pivotGroup`, `pivotChar`, `parseGroup`, and `transformation`.

You will look at each of these fields in turn as you build your configuration file. Additionally, the configuration file that the TOPMed DCC used to parse the freeze 5 data release is included as an example in the WGSAParsr package.

The configuration file you'll make in this workshop will not be as extensive or complicated as the example, but if you'd like, you can load it into your working session to examine:
```{r}
freeze_5_config <- load_config(wgsaparsr_example("fr_5_config.tsv"))
```

Recall that our objective is to aggregate the variants into genic units and to apply filters to restrict the variants in aggreagation units by predicted functional consequence. To achieve this, we will use the following annotation fields: `CHROM`, `POS`, `REF`, `ALT`, `VEP_ensembl_Gene_ID`, `VEP_ensembl_Consequence`, and `CADD_phred`. This list of fields is the first variable we'll need for the configuration file:
```{r}
field <-  c("CHROM",
            "POS",
            "REF",
            "ALT",
            "VEP_ensembl_Gene_ID",
            "VEP_ensembl_Consequence",
            "CADD_phred")
```

The next required variable is `SNV`, a logical value indiciating whether these fields are present in the SNV annotation file. In this case, all of the fields are present in SNV annotation:
```{r}
SNV <- c(rep(TRUE, 7))
```

Then comes `indel`, a logical value indicating whether these fields are present in the indel annotation file. In this case, all of the fields are present in indel annotation:
```{r}
indel <- c(rep(TRUE, 7))
```

[dbNSFP](http://varianttools.sourceforge.net/Annotation/DbNSFP) is an annotation resource included in WGSA. These annotation fields reference gene-oriented features rather than transcript-oriented features, so must be parsed separately if needed for analysis. We do not need to use any dbNSFP variables in this exercise, so the `dbnsfp` variable is FALSE in our configuration:
```{r}
dbnsfp <- c(rep(FALSE, 7))
```

The next two variables are related to the pivoting of annotation list-fields to make them "tidy". Recall that some annotation fields have many values within a single tab-separated field. For example, there is a variant on chromosome 22 at position 15699830 that is annotated with this VEP_ensembl_Gene_ID: `ENSG00000198062|ENSG00000236666|ENSG00000212216|ENSG00000198062|ENSG00000198062` and this VEP_ensembl_Consequence: `intron_variant,NMD_transcript_variant|non_coding_transcript_exon_variant,non_coding_transcript_variant|upstream_gene_variant|intron_variant|intron_variant`. Such list-fields are awkward to work with, so they should be split into 5 lines, with the corresponding fields on the same line (e.g. the first VEP_ensembl_Gene_ID entry and the first VEP_ensembl_Consequence should go together).

This is specified in the configuration file using the `pivotGroup` variable and the `pivotChar` variable is used to specify the character that is the list delimiter - `|` in this case. Build the `pivotGroup` and `pivotChar` variables like this for your configuration:
```{r}
pivotGroup <- c(rep(NA, 4), rep(as.integer(1), 2), NA)

pivotChar <- c(rep(NA, 4), rep("|", 2), NA)
```

The final required variables for the configuration are `parseGroup`, and `transformation`. `parseGroup` defines sets of annotation fields that should be modified together (this primarily applies to sets of dbnsfp annotation fields), and `transformation` defines the modification that should happen. Valid values for `transformation` include `max`, `min`, `pick_Y`, `pick_N`, `pick_A`, `clean`, and `distinct`. 

*WGSAParsr* applies the specified transformation to the field specified in the configuration file, and selects the corresponding value from other fields in the same parseGroup. No transformation or parseGroup is needed for this exercise, but to give an example for completeness, position 21791443 of chromosome 22 has a variant from reference A, the alternative AACAT. This variant is annotated with this Eigen_PC_raw value: `.{2}-0.08822322727842{1}-0.0955006471597487{1}`. A `transformation` of `max` would select the maximum numeric value of this annotation field - the value likely to have the most functional impact - of -0.08822322727842. If there were another field with corresponding entries, such as that variant's Eigen_raw value: `.{2}-0.27473415163451{1}-0.313313344373439{1}`, membership in the same parseGroup would pick the value in that field that corresponded to the maximum value in the Eigen_PC_raw annotation - in this case, -0.27473415163451. Note that the transformation would _not_ return the maximum Eigen_raw value in this case. 

Other possible transformations include the following: `min` selects the minimum value, `pick_Y` picks the character `Y` if present,`pick_N` picks the character `N` if present, `pick_A` picks the character `A` (used for the MutationTaster_pred annotation), `clean` removes unneeded bracketed number strings (used for Ensembl_Regulatory_Build_feature_type, hESC_Topological_Domain, and IMR90_Topological_Domain fields), and `distinct` splits a field to disticnt values (used for Ensembl_Regulatory_Build_TFBS).

None of these transformations are needed for our filtering and aggregation, so we can define the variables for the configuration this way:
```{r}
parseGroup <- c(rep(as.integer(NA), 7))
transformation <- c(rep(as.character(NA), 7))
```

Finally, add an optional configuration variable: `order`. The `order` variable spcifies the column-order in the output file - this is particularly useful if you're working with many annotations, but want to make sure that `CHROM`, `POS`, `REF`, and `ALT` are at the beginning. Note: when using `order` un-numbered fields will go after numbered fields.
```{r}
order <- seq(1,7)
```

Put the configuration variables together in a tibble for validation and saving (a [tibble](https://tibble.tidyverse.org/) is a particular kind of data frame):
```{r}
my_config <-
  tibble(
    field,
    SNV,
    indel,
    dbnsfp,
    pivotGroup,
    pivotChar,
    parseGroup,
    transformation,
    order
  )
```

Now inspect the configuration tibble:
```{r}
my_config
```

*WGSAParsr* includes a configuration validation function, `validate_config()`. A valid configuration tibble or file should get no errors from the validation function:
```{r}
validate_config(my_config)
```

save my_config
```{r}
write_tsv(my_config, "data/my_config.tsv")
```

```{r}
# a bit of cleanup needed for the workshop
if (file.exists("data/snv_parsed.tsv"))(file.remove("data/snv_parsed.tsv"))
if (file.exists("data/indel_parsed.tsv"))(file.remove("data/indel_parsed.tsv"))
```

#### Parsing with WGSAParsr

parse the example files
```{r}
parse_to_file(source_file = snvfile, 
              config = "data/my_config.tsv",
              destination = "data/snv_parsed.tsv",
              chunk_size = 100,
              verbose = FALSE)

parse_to_file(source_file = indelfile, 
              config = "data/my_config.tsv",
              destination = "data/indel_parsed.tsv",
              chunk_size = 100,
              verbose = FALSE)
```

Although the output file has fewer columns than the the raw WGSA output file, this .tsv file is still not particularly nice to work with directly:
```{r}
readLines("data/snv_parsed.tsv", n=2)
```

But `get_fields()` works as expected on the parsed file:
```{r}
# list all fields in an annotation file: 
get_fields("data/snv_parsed.tsv")
```

And in this case, the parsed files are small enough that we can load them into the R session and work with the resulting dataframes for subsequent analysis. (At full scale, the TOPMed DCC imports the parsed files to a database, and uses the annotation data that way):
```{r}
snv_annotation <- read_tsv("data/snv_parsed.tsv",
                           col_types = cols(
                             CHROM = col_character(),
                             POS = col_integer(),
                             REF = col_character(),
                             ALT = col_character(),
                             VEP_ensembl_Gene_ID = col_character(),
                             VEP_ensembl_Consequence = col_character(),
                             CADD_phred = col_double()
                           ))
```
Since there are warnings on that loading, check them out:
```{r}
problems(snv_annotation)
```

Ah, so not really anything to worry about - "." values will be replaced with NA when casting to type double. Go ahead and read the indel file:
```{r}
indel_annotation <- read_tsv("data/indel_parsed.tsv",
                             col_types = cols(
                             CHROM = col_character(),
                             POS = col_integer(),
                             REF = col_character(),
                             ALT = col_character(),
                             VEP_ensembl_Gene_ID = col_character(),
                             VEP_ensembl_Consequence = col_character(),
                             CADD_phred = col_double()
                           ))
```

And since that's fine, go ahead and put them together for subsequent analysis:
```{r}
combined_annotation <- bind_rows(snv_annotation, indel_annotation)
```

### Aggregating and filtering variants using annotation

With the now-tidy variant annotation, the process of aggregating and filtering variants for association testing is almost trivial. For example, an analyst could remove variants that are not associated with a Gene, group the variants by gene, and filter the variants for intron_variants with a CADD_phred score greater than 3 in just a few lines of code:
```{r}
combined_annotation %>% 
  filter(VEP_ensembl_Gene_ID != ".") %>% # remove variants not annotated with a Gene_ID
  group_by(VEP_ensembl_Gene_ID) %>% # aggregate by gene
  filter(CADD_phred > 3) %>% # filter variants to keep only CADD_phred greater than 3
  filter(str_detect(VEP_ensembl_Consequence, "intron_variant")) %>% # keep intron variants
  glimpse() # view the result - 592 variants
```

Now that you've got a set of variants that you can aggregate into genic units, the tibble needs to be reformatted for input to the GENESIS analysis pipeline. The input to the GENESIS pipeline is a data frame with variables called `group_id`, `chr`, `pos`, `ref`, and `alt`. Prepare this data frame and save it for testing (You do not need to filter the variants for this exercise):
```{r}
aggregates <-
  combined_annotation %>%
  filter(VEP_ensembl_Gene_ID != ".") %>% # remove variants not annotated with a Gene_ID
  group_by(VEP_ensembl_Gene_ID) %>% # aggregate by gene
  select(group_id = VEP_ensembl_Gene_ID,
         chr = CHROM,
         pos = POS,
         ref = REF,
         alt = ALT) %>%
  glimpse # inspect the tibble
```

This set can be saved for futher analysis, if you'd like.
```{r}
save(aggregates, file = "data/chr_22_by_gene.RData")
```

You can also compute some summary information about these aggregates, such as counting how many genic units we're using:
```{r}
distinct(as.tibble(aggregates$group_id))
```

We can look at the distribution of the number of variants per aggregation unit:
```{r plot_agg_units}
counts <- aggregates %>% group_by(group_id) %>% summarize(n = n())
ggplot(counts, aes(x = n)) + geom_bar()
```

Feel free to look at other summary statistics and do other exploratory data analysis as you'd like!

### Aggregate unit for association testing exercise

Now you can proceed to an assocation testing exercise. You will be using a slightly different gene-based aggregation unit for the assocation testing exercise. As before, this analysis uses a subset of the TOPMed SNP variants that are present in the 1000 Genomes Project. However, in this exercise, the genic units include TOPMed SNP variants from all chromosomes (no indels, and not just chromosome 22 as before), each genic unit is expanded to include the set of TOPMed SNP variants falling within a GENCODE-defined gene along with 20 kb flanking regions upstream and downstream of that range, and the variants will be from TOPMed freeze 4 instead of freeze 5 (so that the annotation positions are consistent with the build used for genotyping data in the workshop). This set of aggregation units is not filtered by CADD score or consequence.

As before, the aggregation units are defined in an R dataframe. Each row of the dataframe specifies a variant (chr, pos, ref, alt) and the group identifier (group_id) it is a part of. Mutiple rows with different group identifiers can be specified to assign a variant to different groups (a variant can be assigned to mutiple genes).

Begin by loading the aggregation units using `TopmedPipeline::getobj()`:
```{r agg_unit}
workshop.path <- "https://github.com/UW-GAC/topmed_workshop_2018/raw/master/"
aggfile <- "data/variants_by_gene.RData"
if (!file.exists(aggfile)) download.file(file.path(workshop.path, aggfile), aggfile)
aggunit <- TopmedPipeline::getobj(aggfile)
names(aggunit)
head(aggunit)

# an example of variant that is present in mutiple groups
mult <- aggunit %>%
    group_by(chr, pos) %>%
    summarise(n=n()) %>%
    filter(n > 1)
inner_join(aggunit, mult[2,1:2])
```

### Association testing with aggregate units

We can run a burden test or SKAT on each of these units using `assocTestAggregate`. We define a `SeqVarListIterator` object where each list element is an aggregate unit. The constructor expects a `GRangesList`, so we use the TopmedPipeline function `aggregateGRangesList` to quickly convert our single dataframe to the required format. This function can account for multiallelic variants (the same chromosome, position, and ref, but different alt alleles).

```{r aggVarList}
library(TopmedPipeline)
library(SeqVarTools)
if (exists("seqData")) {
    seqResetFilter(seqData, verbose=FALSE)
} else {
    data.path <- "https://github.com/UW-GAC/analysis_pipeline/raw/master/testdata"
    gdsfile <- "data/1KG_phase3_subset_chr1.gds"
    if (!file.exists(gdsfile)) download.file(file.path(data.path, basename(gdsfile)), gdsfile)
    gds <- seqOpen(gdsfile)
    workshop.path <- "https://github.com/UW-GAC/topmed_workshop_2018/raw/master/"
    annotfile <- "data/sample_phenotype_annotation.RData"
    if (!file.exists(annotfile)) download.file(file.path(workshop.path, annotfile), annotfile)
    annot <- getobj(annotfile)
    seqData <- SeqVarData(gds, sampleData=annot)
}
    
# subset to chromosome 1
aggunit <- filter(aggunit, chr == 1)
aggVarList <- aggregateGRangesList(aggunit)
length(aggVarList)
head(names(aggVarList))
aggVarList[[1]]

iterator <- SeqVarListIterator(seqData, variantRanges=aggVarList, verbose=FALSE)
```

As in the previous section, we must fit the null model before running the association test.

```{r assoc_aggregate}
if (!exists("nullmod")) {
    nmfile <- "data/null_model.RData"
    if (!file.exists(nmfile)) download.file(file.path(workshop.path, nmfile), nmfile)
    nullmod <- getobj(nmfile)
}

library(GENESIS)
assoc <- assocTestAggregate(iterator, nullmod, test="Burden", AF.max=0.1, weight.beta=c(1,1))
names(assoc)
head(assoc$results)
head(names(assoc$variantInfo))
head(assoc$variantInfo[[1]])

qqPlot(assoc$results$Score.pval)
```


### Exercise

Since we are working with a subset of the data, many of the genes listed in `group_id` have a very small number of variants. Create a new set of units based on position rather than gene name, using the TopmedPipeline function `aggregateGRanges`. Then run SKAT using those units and a `SeqVarRangeIterator`.

```{r exercise_aggregate}
agg2 <- aggunit %>%
    mutate(chr=factor(chr, levels=c(1:22, "X"))) %>%
    select(chr, pos) %>%
    distinct() %>%
    group_by(chr) %>%
    summarise(min=min(pos), max=max(pos))
head(agg2)

aggByPos <- bind_rows(lapply(1:nrow(agg2), function(i) {
    data.frame(chr=agg2$chr[i],
               start=seq(agg2$min[i], agg2$max[i]-1e6, length.out=10),
               end=seq(agg2$min[i]+1e6, agg2$max[i], length.out=10))
})) %>%
    mutate(group_id=1:n())
head(aggByPos)

aggVarList <- aggregateGRanges(aggByPos)
aggVarList[1:2]

seqResetFilter(seqData, verbose=FALSE)
iterator <- SeqVarRangeIterator(seqData, variantRanges=aggVarList, verbose=FALSE)
assoc <- assocTestAggregate(iterator, nullmod, test="SKAT", AF.max=0.1, weight.beta=c(1,25))
head(assoc$results)
```

```{r assoc_close}
seqClose(gds)
```
